{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/leimao/Two_Layer_Hierarchical_Softmax_PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "# import data\n",
    "# import model\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annakuzmina/anaconda3/lib/python3.5/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/annakuzmina/anaconda3/lib/python3.5/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "model_saved = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_saved['model']\n",
    "encoder = model_saved['encoder']\n",
    "hierarchical_softmax = model_saved['decoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module(\"encoder\", encoder)\n",
    "model.add_module(\"decoder\", hierarchical_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "path = './data/ptb'\n",
    "corpus = data.Corpus(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, 128)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, evaluation=False):\n",
    "    seq_len = min(10, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    #encoder.eval()\n",
    "    model.eval()\n",
    "    #decoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, 10):\n",
    "        data, targets = get_batch(data_source, i, evaluation=True)\n",
    "        emb = encoder(data)\n",
    "        output, hidden = model(emb, hidden)\n",
    "\n",
    "        probs = hierarchical_softmax(output.view(-1, output.size(2)), targets)\n",
    "\n",
    "        loss = -torch.mean(torch.log(probs))\n",
    "\n",
    "        total_loss += len(data) * loss.data\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.832055081514371"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVDSoftmax(A, h, b, W_size, N_size, B, V_t):\n",
    "    h = torch.mm(V_t,h)\n",
    "    z = torch.zeros_like(b)\n",
    "    torch.add(torch.mm(B[:, :W_size], h[:W_size]), b, out=z)\n",
    "\n",
    "    top_k_ind = torch.topk(z, k=N_size, dim=0, sorted=False)[1][:,0]\n",
    "    torch.add(torch.mm(B[top_k_ind], h), b[top_k_ind], out=z[top_k_ind])\n",
    "    z_exp = torch.exp(z - torch.max(z))\n",
    "    return z_exp / z_exp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 150, 100])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.layer_bottom_W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.layer_top_W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierarchicalSoftmax(\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HierarchicalSoftmaxWithSVD(nn.Module):\n",
    "    def __init__(self, ntokens, nhid, ntokens_per_class = None):\n",
    "        super(HierarchicalSoftmax, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.ntokens = ntokens\n",
    "        self.nhid = nhid\n",
    "\n",
    "        if ntokens_per_class is None:\n",
    "            ntokens_per_class = int(np.ceil(np.sqrt(ntokens)))\n",
    "\n",
    "        self.ntokens_per_class = ntokens_per_class\n",
    "\n",
    "        self.nclasses = int(np.ceil(self.ntokens * 1. / self.ntokens_per_class))\n",
    "        self.ntokens_actual = self.nclasses * self.ntokens_per_class\n",
    "\n",
    "        self.layer_top_W = nn.Parameter(torch.FloatTensor(self.nhid, self.nclasses), requires_grad=True)\n",
    "        self.layer_top_b = nn.Parameter(torch.FloatTensor(self.nclasses), requires_grad=True)\n",
    "\n",
    "        self.layer_bottom_W = nn.Parameter(torch.FloatTensor(self.nclasses, self.nhid, self.ntokens_per_class), requires_grad=True)\n",
    "        self.layer_bottom_b = nn.Parameter(torch.FloatTensor(self.nclasses, self.ntokens_per_class), requires_grad=True)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        initrange = 0.1\n",
    "        self.layer_top_W.data.uniform_(-initrange, initrange)\n",
    "        self.layer_top_b.data.fill_(0)\n",
    "        self.layer_bottom_W.data.uniform_(-initrange, initrange)\n",
    "        self.layer_bottom_b.data.fill_(0)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, labels = None):\n",
    "\n",
    "        batch_size, d = inputs.size()\n",
    "\n",
    "        if labels is not None:\n",
    "\n",
    "            label_position_top = labels / self.ntokens_per_class\n",
    "            label_position_bottom = labels % self.ntokens_per_class\n",
    "\n",
    "            layer_top_logits = torch.matmul(inputs, self.layer_top_W) + self.layer_top_b\n",
    "            layer_top_probs = self.softmax(layer_top_logits)\n",
    "\n",
    "            layer_bottom_logits = torch.squeeze(torch.bmm(torch.unsqueeze(inputs, dim=1), self.layer_bottom_W[label_position_top]), dim=1) + self.layer_bottom_b[label_position_top]\n",
    "            layer_bottom_probs = self.softmax(layer_bottom_logits)\n",
    "\n",
    "            target_probs = layer_top_probs[torch.arange(batch_size).long(), label_position_top] * layer_bottom_probs[torch.arange(batch_size).long(), label_position_bottom]\n",
    "\n",
    "            return target_probs\n",
    "\n",
    "        else:\n",
    "            # Remain to be implemented\n",
    "            layer_top_logits = torch.matmul(inputs, self.layer_top_W) + self.layer_top_b\n",
    "            layer_top_probs = self.softmax(layer_top_logits)\n",
    "\n",
    "            word_probs = layer_top_probs[:,0] * self.softmax(torch.matmul(inputs, self.layer_bottom_W[0]) + self.layer_bottom_b[0])\n",
    "\n",
    "            for i in range(1, self.nclasses):\n",
    "                word_probs = torch.cat((word_probs, layer_top_probs[:,i] * self.softmax(torch.matmul(inputs, self.layer_bottom_W[i]) + self.layer_bottom_b[i])), dim=1)\n",
    "\n",
    "            return word_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
