{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/leimao/Two_Layer_Hierarchical_Softmax_PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data\n",
    "\n",
    "import data\n",
    "path = './data/ptb'\n",
    "corpus = data.Corpus(path)\n",
    "\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, 128)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "\n",
    "def get_batch(source, i, evaluation=False):\n",
    "    seq_len = min(10, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def evaluate(model, data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    #encoder.eval()\n",
    "    model.eval()\n",
    "    #decoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, 10):\n",
    "        data, targets = get_batch(data_source, i, evaluation=True)\n",
    "        emb = encoder(data)\n",
    "        output, hidden = model(emb, hidden)\n",
    "\n",
    "        probs = hierarchical_softmax(output.view(-1, output.size(2)), targets)\n",
    "\n",
    "        loss = -torch.mean(torch.log(probs))\n",
    "\n",
    "        total_loss += len(data) * loss.data\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.832055081514371"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "# import data\n",
    "# import model\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annakuzmina/anaconda3/lib/python3.5/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/annakuzmina/anaconda3/lib/python3.5/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "model_saved = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_saved['model']\n",
    "encoder = model_saved['encoder']\n",
    "hierarchical_softmax = model_saved['decoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module(\"encoder\", encoder)\n",
    "model.add_module(\"decoder\", hierarchical_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVDSoftmax(A, h, b, W_size, N_size, B, V_t):\n",
    "    h = torch.mm(V_t,h)\n",
    "    z = torch.zeros_like(b)\n",
    "    torch.add(torch.mm(B[:, :W_size], h[:W_size]), b, out=z)\n",
    "\n",
    "    top_k_ind = torch.topk(z, k=N_size, dim=0, sorted=False)[1][:,0]\n",
    "    torch.add(torch.mm(B[top_k_ind], h), b[top_k_ind], out=z[top_k_ind])\n",
    "    z_exp = torch.exp(z - torch.max(z))\n",
    "    return z_exp / z_exp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utils.HierarchicalSoftmax"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.HierarchicalSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bmm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVDSoftmax(A, h, b, W_size, N_size, B, V_t):\n",
    "    h = torch.matmul(V_t, h)\n",
    "    z = torch.zeros_like(b)\n",
    "    torch.add(torch.matmul(B[:, :W_size], h[:W_size]), b, out=z)\n",
    "\n",
    "    top_k_ind = torch.topk(z, k=N_size, dim=0, sorted=False)[1][:,0]\n",
    "    torch.add(torch.matmul(B[top_k_ind], h), b[top_k_ind], out=z[top_k_ind])\n",
    "    z_exp = torch.exp(z - torch.max(z))\n",
    "    return z_exp / z_exp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, d = inputs.size()\n",
    "label_position_top = labels / self.ntokens_per_class\n",
    "label_position_bottom = labels % self.ntokens_per_class\n",
    "\n",
    "layer_top_logits = torch.matmul(inputs, self.layer_top_W) + self.layer_top_b\n",
    "layer_top_probs = self.softmax(layer_top_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDSoftmax(nn.Module):\n",
    "    def __init__(self, weights, bias):\n",
    "        super(SVDSoftmax, self).__init__()\n",
    "        self.A = weights\n",
    "        self.W_size, self.N_size = int(self.A.shape[1] / 8), int(self.A.shape[0] / 10)\n",
    "        self.b = bias\n",
    "        \n",
    "        U, S, V = torch.svd(weights)\n",
    "        self.V_t = V.transpose(0, 1)\n",
    "        self.B = U * S\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        h = torch.matmul(self.V_t, inputs)\n",
    "        z = torch.zeros_like(self.b)\n",
    "        print(h.shape, z.shape, self.B.shape)\n",
    "        z = torch.matmul(self.B[:, :self.W_size], h[:self.W_size])\n",
    "        z = z + self.b\n",
    "#         torch.add(torch.mm(self.B.data[:, :self.W_size], h[:self.W_size]), self.b.data, out=z.data)\n",
    "\n",
    "        top_k_ind = torch.topk(z, k=self.N_size, dim=0, sorted=False)[1][:,0]\n",
    "        torch.add(torch.mm(self.B[top_k_ind], h), self.b[top_k_ind], out=z[top_k_ind])\n",
    "        z_exp = torch.exp(z - torch.max(z, dim=-1))\n",
    "        return z_exp / z_exp.sum(dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSoftmaxWithSVD(utils.HierarchicalSoftmax):\n",
    "    def __init__(self, pretrained_hierarchical_softmax):\n",
    "        super(HierarchicalSoftmaxWithSVD, self).__init__(pretrained_hierarchical_softmax.ntokens,\n",
    "                                                         pretrained_hierarchical_softmax.nhid,\n",
    "                                                         pretrained_hierarchical_softmax.ntokens_per_class)\n",
    "    \n",
    "        \n",
    "        self.load_state_dict(pretrained_hierarchical_softmax.state_dict())        \n",
    "        self.svd_softmax_top_W = SVDSoftmax(self.layer_top_W, self.layer_top_b)\n",
    "        \n",
    "    def forward(self, inputs, labels = None):\n",
    "\n",
    "        batch_size, d = inputs.size()\n",
    "\n",
    "        if labels is not None:\n",
    "            \n",
    "\n",
    "            label_position_top = labels / self.ntokens_per_class\n",
    "            label_position_bottom = labels % self.ntokens_per_class\n",
    "            \n",
    "#             layer_top_logits = torch.matmul(inputs, self.layer_top_W) + self.layer_top_b\n",
    "#             layer_top_probs = self.softmax(layer_top_logits)\n",
    "            print(\"SVD\")\n",
    "            layer_top_probs = self.svd_softmax_top_W(inputs)\n",
    "\n",
    "            layer_bottom_logits = torch.squeeze(torch.bmm(torch.unsqueeze(inputs, dim=1), self.layer_bottom_W[label_position_top]), dim=1) + self.layer_bottom_b[label_position_top]\n",
    "            layer_bottom_probs = self.softmax(layer_bottom_logits)\n",
    "\n",
    "            target_probs = layer_top_probs[torch.arange(batch_size).long(), label_position_top] * layer_bottom_probs[torch.arange(batch_size).long(), label_position_bottom]\n",
    "\n",
    "            return target_probs\n",
    "\n",
    "        else:\n",
    "            print(\"NO SVD\")\n",
    "            # Remain to be implemented\n",
    "            layer_top_logits = torch.matmul(inputs, self.layer_top_W) + self.layer_top_b\n",
    "            layer_top_probs = self.softmax(layer_top_logits)\n",
    "\n",
    "            word_probs = layer_top_probs[:,0] * self.softmax(torch.matmul(inputs, self.layer_bottom_W[0]) + self.layer_bottom_b[0])\n",
    "\n",
    "            for i in range(1, self.nclasses):\n",
    "                word_probs = torch.cat((word_probs, layer_top_probs[:,i] * self.softmax(torch.matmul(inputs, self.layer_bottom_W[i]) + self.layer_bottom_b[i])), dim=1)\n",
    "\n",
    "            return word_probs\n",
    "        \n",
    "        \n",
    "\n",
    "#         self.init_weights(hierarchical_softmax.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 100])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchical_softmax.l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_decoder = HierarchicalSoftmaxWithSVD(hierarchical_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = svd_model.init_hidden(eval_batch_size)\n",
    "data, targets = get_batch(val_data, 0, evaluation=True)\n",
    "emb = encoder(data)\n",
    "output, hidden = svd_model(emb, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model = model_saved['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model.add_module(\"encoder\", encoder)\n",
    "svd_model.add_module(\"decoder\", svd_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (drop): Dropout(p=0.0)\n",
       "  (rnn): GRU(100, 150)\n",
       "  (encoder): Word2VecEncoder(\n",
       "    (drop): Dropout(p=0.0)\n",
       "    (encoder): Embedding(10000, 100)\n",
       "  )\n",
       "  (decoder): HierarchicalSoftmaxWithSVD(\n",
       "    (softmax): Softmax()\n",
       "    (svd_softmax_top_W): SVDSoftmax(\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_svd(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    #encoder.eval()\n",
    "    svd_model.eval()\n",
    "    #decoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = svd_model.init_hidden(eval_batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, 10):\n",
    "        data, targets = get_batch(data_source, i, evaluation=True)\n",
    "        emb = encoder(data)\n",
    "        output, hidden = svd_model(emb, hidden)\n",
    "\n",
    "        probs = svd_decoder(output.view(-1, output.size(2)), targets)\n",
    "\n",
    "        loss = -torch.mean(torch.log(probs))\n",
    "\n",
    "        total_loss += len(data) * loss.data\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss[0] / len(data_source)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
