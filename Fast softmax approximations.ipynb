{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax fast approximations\n",
    "\n",
    "Короче, выбрал этот странный mxnet т.к. там 1)очень простая предобработка (причем она есть для всех датасетов, стоит только поменять dataset_name) и 2)есть предобученные языковые модели на этих датасетах, так что можно не тратить время на обучение вообще - никаких gpu не надо\n",
    "\n",
    "По сути тут все операции как в numpy (вместо np пиши nd и все)\n",
    "\n",
    "Про gluonnlp & mxnet за 5 минут https://beta.mxnet.io/guide/crash-course/index.html\n",
    "\n",
    "Пока есть только подсчет времени работы произвольной сети (в evaluate)\n",
    "\n",
    "Надо:\n",
    "\n",
    "1. написать таки SVD Softmax слой\n",
    "2. **(contribute here, please)** hierarchical softmax layer (как минимум) ну или чего еще из related work у нас есть - если видите что просто реализовывается - попробуйте\n",
    "3. графички зависимости от W времени и качества (перплексии) для SVD-softmax\n",
    "4. кто-то будет презентовать это в четверг (почти наверное я не смогу)\n",
    "\n",
    "\n",
    "* текущий код работает на cpu, на gpu тоже работал, но я сейчас это снес, если надо будет (зачем?) - напишите мне\n",
    "* 1 & 3 пункты я по-любому сам доделаю в среду\n",
    "* код ниже в принципе не важен (вам его вряд ли стоит запускать) - все что требуется - напишите слои для аппроксимации softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.utils import download\n",
    "from mxnet import ndarray as nd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 0\n",
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else mx.cpu()\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20 #* len(context)\n",
    "lr = 20\n",
    "epochs = 3\n",
    "bptt = 35\n",
    "grad_clip = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wikitext-2'\n",
    "train_dataset, val_dataset, test_dataset = [\n",
    "    nlp.data.WikiText2(\n",
    "        segment=segment, bos=None, eos='<eos>', skip_empty=False)\n",
    "    for segment in ['train', 'val', 'test']\n",
    "]\n",
    "\n",
    "vocab = nlp.Vocab(\n",
    "    nlp.data.Counter(train_dataset), padding_token=None, bos_token=None)\n",
    "\n",
    "bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "    vocab, bptt, batch_size, last_batch='discard')\n",
    "train_data, val_data, test_data = [\n",
    "    bptt_batchify(x) for x in [train_dataset, val_dataset, test_dataset]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(200 -> 33278, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'standard_lstm_lm_200'\n",
    "model, vocab = nlp.model.get_model(model_name, vocab=vocab, \n",
    "                                   dataset_name=dataset_name, pretrained=True, \n",
    "                                   ctx=context)\n",
    "print(model)\n",
    "print(vocab)\n",
    "\n",
    "# model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {\n",
    "    'learning_rate': lr,\n",
    "    'momentum': 0,\n",
    "    'wd': 0\n",
    "})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "standardrnn0_ (\n",
       "  Parameter standardrnn0_hybridsequential0_embedding0_weight (shape=(33278, 200), dtype=float32)\n",
       "  Parameter standardrnn0_lstm0_l0_i2h_weight (shape=(800, 200), dtype=<class 'numpy.float32'>)\n",
       "  Parameter standardrnn0_lstm0_l0_h2h_weight (shape=(800, 200), dtype=<class 'numpy.float32'>)\n",
       "  Parameter standardrnn0_lstm0_l0_i2h_bias (shape=(800,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter standardrnn0_lstm0_l0_h2h_bias (shape=(800,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter standardrnn0_lstm0_l1_i2h_weight (shape=(800, 200), dtype=<class 'numpy.float32'>)\n",
       "  Parameter standardrnn0_lstm0_l1_h2h_weight (shape=(800, 200), dtype=<class 'numpy.float32'>)\n",
       "  Parameter standardrnn0_lstm0_l1_i2h_bias (shape=(800,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter standardrnn0_lstm0_l1_h2h_bias (shape=(800,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter standardrnn0_hybridsequential0_embedding0_bias (shape=(33278,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = model.collect_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33278, 200), (33278,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = params['standardrnn0_hybridsequential0_embedding0_weight'].data().asnumpy()\n",
    "b = params['standardrnn0_hybridsequential0_embedding0_bias'].data().asnumpy()\n",
    "W.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 835 ms, sys: 100 ms, total: 935 ms\n",
      "Wall time: 521 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "U, Sigma, V = np.linalg.svd(W, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33278, 200), (200,), (200, 200))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, Sigma.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    total_time = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        t_begin = time.time()\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = detach(hidden)\n",
    "        L = loss(output.reshape(-3, -1), target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        single_t = time.time() - t_begin\n",
    "        print(f'batch {i} inference time: {single_t:.7f}')\n",
    "        total_time += single_t\n",
    "        ntotal += target.reshape(-1).size\n",
    "    return total_L / ntotal, total_time / ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 inference time: 0.3381550\n",
      "batch 1 inference time: 0.2853339\n",
      "batch 2 inference time: 0.2853391\n",
      "batch 3 inference time: 0.2871799\n",
      "batch 4 inference time: 0.2963479\n",
      "batch 5 inference time: 0.3045039\n",
      "batch 6 inference time: 0.2889631\n",
      "batch 7 inference time: 0.3018198\n",
      "batch 8 inference time: 0.3025203\n",
      "batch 9 inference time: 0.2889941\n",
      "batch 10 inference time: 0.2942410\n",
      "batch 11 inference time: 0.2948592\n",
      "batch 12 inference time: 0.2909350\n",
      "batch 13 inference time: 0.4573598\n",
      "batch 14 inference time: 0.2910721\n",
      "batch 15 inference time: 0.3886399\n",
      "batch 16 inference time: 0.2938199\n",
      "batch 17 inference time: 0.2966020\n",
      "batch 18 inference time: 0.2973442\n",
      "batch 19 inference time: 0.2957962\n",
      "batch 20 inference time: 0.2886200\n",
      "batch 21 inference time: 0.2932250\n",
      "batch 22 inference time: 0.2909870\n",
      "batch 23 inference time: 0.2901282\n",
      "batch 24 inference time: 0.2916729\n",
      "batch 25 inference time: 0.2870910\n",
      "batch 26 inference time: 0.3891699\n",
      "batch 27 inference time: 0.2976661\n",
      "batch 28 inference time: 0.3012989\n",
      "batch 29 inference time: 0.3140190\n",
      "batch 30 inference time: 0.3259528\n",
      "batch 31 inference time: 0.3229072\n",
      "batch 32 inference time: 0.2972920\n",
      "batch 33 inference time: 0.3058021\n",
      "batch 34 inference time: 0.2855480\n",
      "batch 35 inference time: 0.2882628\n",
      "batch 36 inference time: 0.2888446\n",
      "batch 37 inference time: 0.3303990\n",
      "batch 38 inference time: 0.2963011\n",
      "batch 39 inference time: 0.4371598\n",
      "batch 40 inference time: 0.3915370\n",
      "batch 41 inference time: 0.2891738\n",
      "batch 42 inference time: 0.3109031\n",
      "batch 43 inference time: 0.3480461\n",
      "batch 44 inference time: 0.3478098\n",
      "batch 45 inference time: 0.3046260\n",
      "batch 46 inference time: 0.3079791\n",
      "batch 47 inference time: 0.2830191\n",
      "batch 48 inference time: 0.3489141\n",
      "batch 49 inference time: 0.2820349\n",
      "batch 50 inference time: 0.2983260\n",
      "batch 51 inference time: 0.2916360\n",
      "batch 52 inference time: 0.2850571\n",
      "batch 53 inference time: 0.2892151\n",
      "batch 54 inference time: 0.2845020\n",
      "batch 55 inference time: 0.2841291\n",
      "batch 56 inference time: 0.2887712\n",
      "batch 57 inference time: 0.2876213\n",
      "batch 58 inference time: 0.2991056\n",
      "batch 59 inference time: 0.3007479\n",
      "batch 60 inference time: 0.2925262\n",
      "batch 61 inference time: 0.3374770\n",
      "batch 62 inference time: 0.4080200\n",
      "batch 63 inference time: 0.3474870\n",
      "batch 64 inference time: 0.3791120\n",
      "batch 65 inference time: 0.3205929\n",
      "batch 66 inference time: 0.3334818\n",
      "batch 67 inference time: 0.3323581\n",
      "batch 68 inference time: 0.3527088\n",
      "batch 69 inference time: 0.3311028\n",
      "batch 70 inference time: 0.2900901\n",
      "batch 71 inference time: 0.2988501\n",
      "batch 72 inference time: 0.2838693\n",
      "batch 73 inference time: 0.2877572\n",
      "batch 74 inference time: 0.2878981\n",
      "batch 75 inference time: 0.2848151\n",
      "batch 76 inference time: 0.3065190\n",
      "batch 77 inference time: 0.3291628\n",
      "batch 78 inference time: 0.3114400\n",
      "batch 79 inference time: 0.3277400\n",
      "batch 80 inference time: 0.3259721\n",
      "batch 81 inference time: 0.2816980\n",
      "batch 82 inference time: 0.2932899\n",
      "batch 83 inference time: 0.2847750\n",
      "batch 84 inference time: 0.2862580\n",
      "batch 85 inference time: 0.2888100\n",
      "batch 86 inference time: 0.2860351\n",
      "batch 87 inference time: 0.2845607\n",
      "batch 88 inference time: 0.2886569\n",
      "batch 89 inference time: 0.2847381\n",
      "batch 90 inference time: 0.3367889\n",
      "batch 91 inference time: 0.3811359\n",
      "batch 92 inference time: 0.2886710\n",
      "batch 93 inference time: 0.3670321\n",
      "batch 94 inference time: 0.3192878\n",
      "batch 95 inference time: 0.2931180\n",
      "batch 96 inference time: 0.3048718\n",
      "batch 97 inference time: 0.2911491\n",
      "batch 98 inference time: 0.2892737\n",
      "batch 99 inference time: 0.2903411\n",
      "batch 100 inference time: 0.2842479\n",
      "batch 101 inference time: 0.2871022\n",
      "batch 102 inference time: 0.2942300\n",
      "batch 103 inference time: 0.3240242\n",
      "batch 104 inference time: 0.4062338\n",
      "batch 105 inference time: 0.3431320\n",
      "batch 106 inference time: 0.2945280\n",
      "batch 107 inference time: 0.3134611\n",
      "batch 108 inference time: 0.2876422\n",
      "batch 109 inference time: 0.2846320\n",
      "batch 110 inference time: 0.4134331\n",
      "batch 111 inference time: 0.4507170\n",
      "batch 112 inference time: 0.4409337\n",
      "batch 113 inference time: 0.2999990\n",
      "batch 114 inference time: 0.3064532\n",
      "batch 115 inference time: 0.3046710\n",
      "batch 116 inference time: 0.3623660\n",
      "batch 117 inference time: 0.4678788\n",
      "batch 118 inference time: 0.3144379\n",
      "batch 119 inference time: 0.2910132\n",
      "batch 120 inference time: 0.3493333\n",
      "batch 121 inference time: 0.3157339\n",
      "batch 122 inference time: 0.2845809\n",
      "batch 123 inference time: 0.3581500\n",
      "batch 124 inference time: 0.3352399\n",
      "batch 125 inference time: 0.3387587\n",
      "batch 126 inference time: 0.3339369\n",
      "batch 127 inference time: 0.3447750\n",
      "batch 128 inference time: 0.3301170\n",
      "batch 129 inference time: 0.3355820\n",
      "batch 130 inference time: 0.3298702\n",
      "batch 131 inference time: 0.3346879\n",
      "batch 132 inference time: 0.3085389\n",
      "batch 133 inference time: 0.3076189\n",
      "batch 134 inference time: 0.3160722\n",
      "batch 135 inference time: 0.3092740\n",
      "batch 136 inference time: 0.3283739\n",
      "batch 137 inference time: 0.3011189\n",
      "batch 138 inference time: 0.3121381\n",
      "batch 139 inference time: 0.3132727\n",
      "batch 140 inference time: 0.3154488\n",
      "batch 141 inference time: 0.3166251\n",
      "batch 142 inference time: 0.3000550\n",
      "batch 143 inference time: 0.3129418\n",
      "batch 144 inference time: 0.3053081\n",
      "batch 145 inference time: 0.3041308\n",
      "batch 146 inference time: 0.3020129\n",
      "batch 147 inference time: 0.3183439\n",
      "batch 148 inference time: 0.3099427\n",
      "batch 149 inference time: 0.3008273\n",
      "batch 150 inference time: 0.3121290\n",
      "batch 151 inference time: 0.3113291\n",
      "batch 152 inference time: 0.2870390\n",
      "batch 153 inference time: 0.2927330\n",
      "batch 154 inference time: 0.2909091\n",
      "batch 155 inference time: 0.3437381\n",
      "batch 156 inference time: 0.3432689\n",
      "batch 157 inference time: 0.3478920\n",
      "batch 158 inference time: 0.3451629\n",
      "batch 159 inference time: 0.3146701\n",
      "batch 160 inference time: 0.3275499\n",
      "batch 161 inference time: 0.3572769\n",
      "batch 162 inference time: 0.3381660\n",
      "batch 163 inference time: 0.3131719\n",
      "batch 164 inference time: 0.3239589\n",
      "batch 165 inference time: 0.3036010\n",
      "batch 166 inference time: 0.3017049\n",
      "batch 167 inference time: 0.3193729\n",
      "batch 168 inference time: 0.3211999\n",
      "batch 169 inference time: 0.3226781\n",
      "batch 170 inference time: 0.3138602\n",
      "batch 171 inference time: 0.2987411\n",
      "batch 172 inference time: 0.3118792\n",
      "batch 173 inference time: 0.3429530\n",
      "batch 174 inference time: 0.2848251\n",
      "batch 175 inference time: 0.3284690\n",
      "batch 176 inference time: 0.3097157\n",
      "batch 177 inference time: 0.3156130\n",
      "batch 178 inference time: 0.2921171\n",
      "batch 179 inference time: 0.3021250\n",
      "batch 180 inference time: 0.2961249\n",
      "batch 181 inference time: 0.2965162\n",
      "batch 182 inference time: 0.2875831\n",
      "batch 183 inference time: 0.3096881\n",
      "batch 184 inference time: 0.2896922\n",
      "batch 185 inference time: 0.2824121\n",
      "batch 186 inference time: 0.2930470\n",
      "batch 187 inference time: 0.2836580\n",
      "batch 188 inference time: 0.3008211\n",
      "batch 189 inference time: 0.2844062\n",
      "batch 190 inference time: 0.3199627\n",
      "batch 191 inference time: 0.3161831\n",
      "batch 192 inference time: 0.3061688\n",
      "batch 193 inference time: 0.3332319\n",
      "batch 194 inference time: 0.3341351\n",
      "batch 195 inference time: 0.2934730\n",
      "batch 196 inference time: 0.3063061\n",
      "batch 197 inference time: 0.2881181\n",
      "batch 198 inference time: 0.2995641\n",
      "batch 199 inference time: 0.3088131\n",
      "batch 200 inference time: 0.2856021\n",
      "batch 201 inference time: 0.2866943\n",
      "batch 202 inference time: 0.3899248\n",
      "batch 203 inference time: 0.2990770\n",
      "batch 204 inference time: 0.3860888\n",
      "batch 205 inference time: 0.2866349\n",
      "batch 206 inference time: 0.2956469\n",
      "batch 207 inference time: 0.2941310\n",
      "batch 208 inference time: 0.2844701\n",
      "batch 209 inference time: 0.2857678\n",
      "batch 210 inference time: 0.2901168\n",
      "batch 211 inference time: 0.2838299\n",
      "batch 212 inference time: 0.2886090\n",
      "batch 213 inference time: 0.2904818\n",
      "batch 214 inference time: 0.2841220\n",
      "batch 215 inference time: 0.2881052\n",
      "batch 216 inference time: 0.2888529\n",
      "batch 217 inference time: 0.2877610\n",
      "batch 218 inference time: 0.2882111\n",
      "batch 219 inference time: 0.2841232\n",
      "batch 220 inference time: 0.2928898\n",
      "batch 221 inference time: 0.2877328\n",
      "batch 222 inference time: 0.2967222\n",
      "batch 223 inference time: 0.3053241\n",
      "batch 224 inference time: 0.2950587\n",
      "batch 225 inference time: 0.2941980\n",
      "batch 226 inference time: 0.3868487\n",
      "batch 227 inference time: 0.4342179\n",
      "batch 228 inference time: 0.2932739\n",
      "batch 229 inference time: 0.3149631\n",
      "batch 230 inference time: 0.3162670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 231 inference time: 0.3034580\n",
      "batch 232 inference time: 0.3208168\n",
      "batch 233 inference time: 0.2967632\n",
      "batch 234 inference time: 0.3108890\n",
      "batch 235 inference time: 0.3079271\n",
      "batch 236 inference time: 0.3250930\n",
      "batch 237 inference time: 0.3111029\n",
      "batch 238 inference time: 0.2859759\n",
      "batch 239 inference time: 0.3233099\n",
      "batch 240 inference time: 0.3105931\n",
      "batch 241 inference time: 0.3215377\n",
      "batch 242 inference time: 0.3195000\n",
      "batch 243 inference time: 0.2983119\n",
      "batch 244 inference time: 0.3450181\n",
      "batch 245 inference time: 0.3218057\n",
      "batch 246 inference time: 0.2971511\n",
      "batch 247 inference time: 0.2958679\n",
      "batch 248 inference time: 0.2833960\n",
      "batch 249 inference time: 0.2972338\n",
      "batch 250 inference time: 0.2889900\n",
      "batch 251 inference time: 0.2973111\n",
      "batch 252 inference time: 0.2942610\n",
      "batch 253 inference time: 0.2892818\n",
      "batch 254 inference time: 0.3495562\n",
      "batch 255 inference time: 0.3321617\n",
      "batch 256 inference time: 0.2970431\n",
      "batch 257 inference time: 0.2852890\n",
      "batch 258 inference time: 0.2855852\n",
      "batch 259 inference time: 0.2945192\n",
      "batch 260 inference time: 0.2882710\n",
      "batch 261 inference time: 0.2841630\n",
      "batch 262 inference time: 0.2829361\n",
      "batch 263 inference time: 0.2861180\n",
      "batch 264 inference time: 0.2865849\n",
      "batch 265 inference time: 0.2977300\n",
      "batch 266 inference time: 0.2885330\n",
      "batch 267 inference time: 0.2822990\n",
      "batch 268 inference time: 0.2903922\n",
      "batch 269 inference time: 0.2916889\n",
      "batch 270 inference time: 0.2840109\n",
      "batch 271 inference time: 0.2875912\n",
      "batch 272 inference time: 0.2912369\n",
      "batch 273 inference time: 0.2887230\n",
      "batch 274 inference time: 0.2914898\n",
      "batch 275 inference time: 0.2834010\n",
      "batch 276 inference time: 0.2921700\n",
      "batch 277 inference time: 0.2905972\n",
      "batch 278 inference time: 0.2891569\n",
      "batch 279 inference time: 0.2894459\n",
      "batch 280 inference time: 0.2920942\n",
      "batch 281 inference time: 0.2892318\n",
      "batch 282 inference time: 0.2892122\n",
      "batch 283 inference time: 0.2885799\n",
      "batch 284 inference time: 0.2887180\n",
      "batch 285 inference time: 0.2863231\n",
      "batch 286 inference time: 0.2896390\n",
      "batch 287 inference time: 0.2893760\n",
      "batch 288 inference time: 0.2917240\n",
      "batch 289 inference time: 0.2901587\n",
      "batch 290 inference time: 0.2880950\n",
      "batch 291 inference time: 0.2868040\n",
      "batch 292 inference time: 0.2908278\n",
      "batch 293 inference time: 0.2865152\n",
      "batch 294 inference time: 0.2931318\n",
      "batch 295 inference time: 0.2920501\n",
      "batch 296 inference time: 0.2854629\n",
      "batch 297 inference time: 0.2884359\n",
      "batch 298 inference time: 0.2888052\n",
      "batch 299 inference time: 0.2899210\n",
      "batch 300 inference time: 0.2893829\n",
      "batch 301 inference time: 0.2899179\n",
      "batch 302 inference time: 0.2883580\n",
      "batch 303 inference time: 0.2880793\n",
      "batch 304 inference time: 0.2877738\n",
      "batch 305 inference time: 0.2870839\n",
      "batch 306 inference time: 0.2877419\n",
      "batch 307 inference time: 0.2905540\n",
      "batch 308 inference time: 0.2911911\n",
      "batch 309 inference time: 0.2876661\n",
      "batch 310 inference time: 0.2941730\n",
      "batch 311 inference time: 0.2831540\n",
      "batch 312 inference time: 0.2840650\n",
      "batch 313 inference time: 0.2847950\n",
      "batch 314 inference time: 0.2906339\n",
      "batch 315 inference time: 0.2844801\n",
      "batch 316 inference time: 0.2917740\n",
      "batch 317 inference time: 0.2873118\n",
      "batch 318 inference time: 0.2925351\n",
      "batch 319 inference time: 0.2887907\n",
      "batch 320 inference time: 0.3292933\n",
      "batch 321 inference time: 0.3605461\n",
      "batch 322 inference time: 0.3583331\n",
      "batch 323 inference time: 0.3096719\n",
      "batch 324 inference time: 0.3107750\n",
      "batch 325 inference time: 0.3029819\n",
      "batch 326 inference time: 0.3244028\n",
      "batch 327 inference time: 0.3482180\n",
      "batch 328 inference time: 0.3118780\n",
      "batch 329 inference time: 0.2933340\n",
      "batch 330 inference time: 0.2929368\n",
      "batch 331 inference time: 0.2811770\n",
      "batch 332 inference time: 0.2922592\n",
      "batch 333 inference time: 0.2930732\n",
      "batch 334 inference time: 0.2873769\n",
      "batch 335 inference time: 0.2991958\n",
      "batch 336 inference time: 0.3158729\n",
      "batch 337 inference time: 0.3086059\n",
      "batch 338 inference time: 0.3171360\n",
      "batch 339 inference time: 0.3445251\n",
      "batch 340 inference time: 0.2967820\n",
      "batch 341 inference time: 0.3149137\n",
      "batch 342 inference time: 0.3303778\n",
      "batch 343 inference time: 0.3139069\n",
      "batch 344 inference time: 0.3397641\n",
      "batch 345 inference time: 0.3073852\n",
      "batch 346 inference time: 0.3027148\n",
      "batch 347 inference time: 0.3018541\n",
      "batch 348 inference time: 0.3475609\n",
      "batch 349 inference time: 0.3188159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.622447090242347, 0.00044113784322933274)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_data, batch_size, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Softmax, self).__init__(**kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.softmax(x)\n",
    "    \n",
    "class SVDSoftmax(gluon.HybridBlock):\n",
    "    # TODO\n",
    "    def __init__(self, svd_decomposition, bias, **kwargs):\n",
    "        super(Softmax, self).__init__(**kwargs)\n",
    "        U, Sigma, V_t = svd_decomposition\n",
    "        self.B = np.dot(U, nd.diag(Sigma))\n",
    "        self.V_t = V_t\n",
    "        self.b = bias\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33278, 200), (200,), (200, 200))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, Sigma.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_decoder = mx.gluon.nn.HybridSequential()\n",
    "# new_decoder.add(Softmax())\n",
    "# new_decoder.add(gluon.nn.Dense(33278, in_units=200))\n",
    "# new_decoder.initialize()\n",
    "# model.decoder = new_decoder\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_data, batch_size, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_decoder = mx.gluon.nn.HybridSequential()\n",
    "new_decoder.add(mx.gluon.nn.Dense(units=100, activation='relu', flatten=False))\n",
    "new_decoder.initialize()\n",
    "\n",
    "model.decoder = new_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardRNN(\n",
       "  (embedding): HybridSequential(\n",
       "    (0): Embedding(33278 -> 200, float32)\n",
       "    (1): Dropout(p = 0.2, axes=())\n",
       "  )\n",
       "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
       "  (decoder): HybridSequential(\n",
       "    (0): Dense(None -> 100, Activation(relu))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_data, batch_size, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx)\n",
    "                   for ctx in context]\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context,\n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context,\n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output.reshape(-3, -1), y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / (len(context) * X.size)\n",
    "                    Ls.append(batch_L / (len(context) * X.size))\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L),\n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "        val_L, val_time = evaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L, test_time = evaluate(model, test_data, batch_size, context[0])\n",
    "            model.save_parameters('{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f, test time %.2f'%(\n",
    "                test_L, math.exp(test_L), test_time))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) /\n",
    "                            (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 200/2983] loss 7.66, ppl 2129.11, throughput 281.65 samples/s\n",
      "[Epoch 0 Batch 400/2983] loss 6.75, ppl 853.92, throughput 286.11 samples/s\n",
      "[Epoch 0 Batch 600/2983] loss 6.34, ppl 568.07, throughput 286.57 samples/s\n",
      "[Epoch 0 Batch 800/2983] loss 6.18, ppl 483.26, throughput 286.67 samples/s\n",
      "[Epoch 0 Batch 1000/2983] loss 6.05, ppl 422.96, throughput 286.33 samples/s\n",
      "[Epoch 0 Batch 1200/2983] loss 5.97, ppl 391.82, throughput 286.72 samples/s\n",
      "[Epoch 0 Batch 1400/2983] loss 5.87, ppl 352.51, throughput 286.17 samples/s\n",
      "[Epoch 0 Batch 1600/2983] loss 5.86, ppl 351.89, throughput 286.47 samples/s\n",
      "[Epoch 0 Batch 1800/2983] loss 5.70, ppl 299.67, throughput 286.51 samples/s\n",
      "[Epoch 0 Batch 2000/2983] loss 5.67, ppl 289.43, throughput 286.45 samples/s\n",
      "[Epoch 0 Batch 2200/2983] loss 5.57, ppl 261.92, throughput 283.67 samples/s\n",
      "[Epoch 0 Batch 2400/2983] loss 5.58, ppl 264.64, throughput 287.07 samples/s\n",
      "[Epoch 0 Batch 2600/2983] loss 5.56, ppl 259.61, throughput 286.79 samples/s\n",
      "[Epoch 0 Batch 2800/2983] loss 5.45, ppl 233.80, throughput 286.83 samples/s\n",
      "[Epoch 0] throughput 286.10 samples/s\n",
      "[Epoch 0] time cost 220.27s, valid loss 5.43, valid ppl 229.27\n",
      "test loss 5.35, test ppl 209.76\n",
      "[Epoch 1 Batch 200/2983] loss 5.46, ppl 235.52, throughput 285.73 samples/s\n",
      "[Epoch 1 Batch 400/2983] loss 5.45, ppl 231.90, throughput 285.68 samples/s\n",
      "[Epoch 1 Batch 600/2983] loss 5.28, ppl 196.60, throughput 286.16 samples/s\n",
      "[Epoch 1 Batch 800/2983] loss 5.30, ppl 200.04, throughput 285.95 samples/s\n",
      "[Epoch 1 Batch 1000/2983] loss 5.27, ppl 193.67, throughput 286.28 samples/s\n",
      "[Epoch 1 Batch 1200/2983] loss 5.26, ppl 192.25, throughput 287.54 samples/s\n",
      "[Epoch 1 Batch 1400/2983] loss 5.26, ppl 193.11, throughput 287.19 samples/s\n",
      "[Epoch 1 Batch 1600/2983] loss 5.32, ppl 204.93, throughput 287.07 samples/s\n",
      "[Epoch 1 Batch 1800/2983] loss 5.19, ppl 178.72, throughput 286.75 samples/s\n",
      "[Epoch 1 Batch 2000/2983] loss 5.20, ppl 181.86, throughput 286.25 samples/s\n",
      "[Epoch 1 Batch 2200/2983] loss 5.12, ppl 166.78, throughput 286.62 samples/s\n",
      "[Epoch 1 Batch 2400/2983] loss 5.15, ppl 171.82, throughput 286.76 samples/s\n",
      "[Epoch 1 Batch 2600/2983] loss 5.16, ppl 174.33, throughput 286.38 samples/s\n",
      "[Epoch 1 Batch 2800/2983] loss 5.08, ppl 160.75, throughput 285.96 samples/s\n",
      "[Epoch 1] throughput 286.54 samples/s\n",
      "[Epoch 1] time cost 219.95s, valid loss 5.18, valid ppl 178.26\n",
      "test loss 5.11, test ppl 164.85\n",
      "[Epoch 2 Batch 200/2983] loss 5.13, ppl 169.59, throughput 285.19 samples/s\n",
      "[Epoch 2 Batch 400/2983] loss 5.15, ppl 172.81, throughput 285.91 samples/s\n",
      "[Epoch 2 Batch 600/2983] loss 4.98, ppl 145.79, throughput 286.02 samples/s\n",
      "[Epoch 2 Batch 800/2983] loss 5.03, ppl 152.64, throughput 285.52 samples/s\n",
      "[Epoch 2 Batch 1000/2983] loss 5.01, ppl 150.53, throughput 286.92 samples/s\n",
      "[Epoch 2 Batch 1200/2983] loss 5.02, ppl 150.96, throughput 286.73 samples/s\n",
      "[Epoch 2 Batch 1400/2983] loss 5.04, ppl 154.39, throughput 286.80 samples/s\n",
      "[Epoch 2 Batch 1600/2983] loss 5.12, ppl 166.87, throughput 287.14 samples/s\n",
      "[Epoch 2 Batch 1800/2983] loss 4.98, ppl 145.72, throughput 286.81 samples/s\n",
      "[Epoch 2 Batch 2000/2983] loss 5.01, ppl 150.14, throughput 286.90 samples/s\n",
      "[Epoch 2 Batch 2200/2983] loss 4.92, ppl 137.60, throughput 286.56 samples/s\n",
      "[Epoch 2 Batch 2400/2983] loss 4.96, ppl 142.19, throughput 285.76 samples/s\n",
      "[Epoch 2 Batch 2600/2983] loss 4.98, ppl 145.89, throughput 286.74 samples/s\n",
      "[Epoch 2 Batch 2800/2983] loss 4.91, ppl 134.98, throughput 286.45 samples/s\n",
      "[Epoch 2] throughput 286.46 samples/s\n",
      "[Epoch 2] time cost 219.99s, valid loss 5.07, valid ppl 158.99\n",
      "test loss 5.00, test ppl 147.81\n",
      "Total training throughput 255.53 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, val_data, test_data, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
